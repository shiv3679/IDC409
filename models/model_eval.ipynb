{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>sentence</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Beyoncé Giselle Knowles-Carter (/biːˈjɒnseɪ/ b...</td>\n",
       "      <td>sentence</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Born and raised in Houston, Texas, she perform...</td>\n",
       "      <td>sentence</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Managed by her father, Mathew Knowles, the gro...</td>\n",
       "      <td>sentence</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>Their hiatus saw the release of Beyoncé's debu...</td>\n",
       "      <td>sentence</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>Following the disbandment of Destiny's Child i...</td>\n",
       "      <td>sentence</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                                           sentence     label\n",
       "0           0  Beyoncé Giselle Knowles-Carter (/biːˈjɒnseɪ/ b...  sentence\n",
       "1           1  Born and raised in Houston, Texas, she perform...  sentence\n",
       "2           2  Managed by her father, Mathew Knowles, the gro...  sentence\n",
       "3           3  Their hiatus saw the release of Beyoncé's debu...  sentence\n",
       "4           4  Following the disbandment of Destiny's Child i...  sentence"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd \n",
    "import numpy as np \n",
    "\n",
    "file = 'videotranscript.csv'\n",
    "data = pd.read_csv(file)\n",
    "\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                            sentence  label\n",
      "0  beyoncé giselle knowlescarter biːˈjɒnseɪ beeyo...      1\n",
      "1  born and raised in houston texas she performed...      1\n",
      "2  managed by her father mathew knowles the group...      1\n",
      "3  their hiatus saw the release of beyoncés debut...      1\n",
      "4  following the disbandment of destinys child in...      1\n",
      "(235110, 2)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Remove unnecessary index column\n",
    "data.drop(columns=data.columns[0], inplace=True)\n",
    "\n",
    "# Clean text: lowercasing, removing punctuation, and stripping whitespace\n",
    "data['sentence'] = data['sentence'].str.lower().str.replace(r'[^\\w\\s]', '', regex=True).str.strip()\n",
    "\n",
    "# Encode labels\n",
    "label_encoder = LabelEncoder()\n",
    "data['label'] = label_encoder.fit_transform(data['label'])\n",
    "\n",
    "print(data.head())\n",
    "print(data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(235110, 10000)\n",
      "['01' '02' '03' '05' '10' '100' '1000' '10000' '100000' '101930suser17'\n",
      " '1019adultsuser28' '1019adultsuser32' '1019adultsuser35' '102' '105'\n",
      " '106' '107' '10th' '11' '110' '1100' '1106adultsuser105' '110820suser21'\n",
      " '110840suser18' '110840suser7' '110920suser156' '110940suser18'\n",
      " '110940suser34' '110940suser52' '110940suser7' '114' '115' '11th' '12'\n",
      " '120' '1200' '12000' '125' '127' '128' '12th' '13' '130' '1300' '13th'\n",
      " '14' '1400' '1419teens' '1492' '14th' '15' '150' '1500' '15000' '150000'\n",
      " '15th' '16' '160' '1600' '1648' '16th' '17' '1700' '1700s' '1757' '1758'\n",
      " '1762' '1776' '1788' '1789' '1790' '1791' '1793' '1795' '17th' '18' '180'\n",
      " '1800' '1800s' '1801' '1808' '1810' '1812' '1814' '1815' '1817' '1820'\n",
      " '1821' '1822' '1824' '1825' '1829' '1830' '1831' '1832' '1833' '1834'\n",
      " '1835' '1836' '1837']\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Initialize the TF-IDF Vectorizer\n",
    "tfidf_vectorizer = TfidfVectorizer(max_features=10000)  # Limiting to 10,000 features for manageability\n",
    "\n",
    "# Fit and transform the sentences to a TF-IDF matrix\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform(data['sentence'])\n",
    "\n",
    "# Display the shape of the resulting TF-IDF matrix and the type of features it has created\n",
    "print(tfidf_matrix.shape)\n",
    "print(tfidf_vectorizer.get_feature_names_out()[:100])  # Displaying the first 100 features for inspection\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(235110, 300)\n",
      "Explained variance ratio (cumulative): 0.29080739044987475\n"
     ]
    }
   ],
   "source": [
    "from sklearn.decomposition import TruncatedSVD\n",
    "\n",
    "# Initialize Truncated SVD with a specific number of components to reduce the feature space\n",
    "lsa = TruncatedSVD(n_components=300)  # You can adjust the number of components based on your specific needs\n",
    "\n",
    "# Fit and transform the TF-IDF matrix using LSA\n",
    "lsa_matrix = lsa.fit_transform(tfidf_matrix)\n",
    "\n",
    "# Display the shape of the resulting LSA matrix and the amount of variance explained by the components\n",
    "print(lsa_matrix.shape)\n",
    "print(f\"Explained variance ratio (cumulative): {sum(lsa.explained_variance_ratio_)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(235110, 10)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "\n",
    "# Convert TF-IDF to a term-frequency matrix\n",
    "tf_vectorizer = CountVectorizer(max_features=10000)\n",
    "tf_matrix = tf_vectorizer.fit_transform(data['sentence'])\n",
    "\n",
    "# Initialize LDA with a specified number of topics\n",
    "lda = LatentDirichletAllocation(n_components=10, random_state=0)  # You can adjust the number of topics\n",
    "\n",
    "# Fit LDA model on the dataset\n",
    "lda_matrix = lda.fit_transform(tf_matrix)\n",
    "\n",
    "# Check the shape of the LDA matrix\n",
    "print(lda_matrix.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\stormbreaker\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('instance', 0.7486647963523865), ('ideal', 0.6622704267501831), ('explanation', 0.6369928121566772), ('amalgamation', 0.6254404187202454), ('expression', 0.6167770624160767), ('substitute', 0.6009918451309204), ('unknown', 0.595906674861908), ('alloy', 0.5931105017662048), ('abbreviation', 0.5892019867897034), ('jew', 0.5867902040481567)]\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import Word2Vec\n",
    "import gensim.downloader as api\n",
    "from nltk.tokenize import word_tokenize\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "\n",
    "# Tokenize sentences\n",
    "data['tokenized'] = data['sentence'].apply(word_tokenize)\n",
    "\n",
    "# Train a Word2Vec model\n",
    "word2vec_model = Word2Vec(sentences=data['tokenized'], vector_size=100, window=5, min_count=5, workers=4)\n",
    "\n",
    "# Optionally, you can use a pre-trained model instead\n",
    "# word2vec_model = api.load('word2vec-google-news-300')\n",
    "\n",
    "# Explore the model\n",
    "print(word2vec_model.wv.most_similar('example'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\stormbreaker\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# Load a pre-trained sentence embedding model\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "# Generate sentence embeddings\n",
    "sentence_embeddings = model.encode(data['sentence'].tolist())\n",
    "\n",
    "# Check the shape of the embeddings\n",
    "print(sentence_embeddings.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
